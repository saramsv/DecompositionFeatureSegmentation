{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "damaged-italy",
   "metadata": {},
   "source": [
    "# Make odgt files \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-inflation",
   "metadata": {},
   "source": [
    "## Make odgt files from clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpha-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odgt_format_from_dir(filename, labeled_img_dir, all_img_dir, labels_dir, dest_filename, extension):\n",
    "    df_seqs = pd.read_csv(filename , sep=\":\", names=['img', 'label'])\n",
    "    with open(dest_filename + \".odgt\", \"w\") as fw:\n",
    "\n",
    "        for img in glob.glob(labeled_img_dir + '/*.jpg'):\n",
    "            img_name = img.split(\"/\")[-1]\n",
    "            img_path = os.path.join(all_img_dir, img_name)\n",
    "            w, h = cv2.imread(img_path).shape[:2]\n",
    "            cluster_name = df_seqs[df_seqs['img'].str.contains(img_name)].reset_index(drop=True)['label'][0]\n",
    "            seq_match = df_seqs[df_seqs['label'] == cluster_name].reset_index(drop=True)\n",
    "            if seq_match.shape[0] >= 2:\n",
    "                img_names = [img_path]\n",
    "                seg = labels_dir + img_name.split('.')[0] + '.png'\n",
    "                seg_names = [seg] * 2\n",
    "                for row in seq_match.iterrows():\n",
    "                    if row[1].img not in img_names:\n",
    "                        img_names.append(row[1].img)\n",
    "                        w, h = cv2.imread(row[1].img).shape[:2]\n",
    "                    if len(img_names) == 2:\n",
    "                        new_line = {}\n",
    "                        new_line[\"fpath_img\"], new_line[\"fpath_segm\"], new_line[\"width\"], new_line[\"height\"] = \\\n",
    "                                img_names, seg_names, w, h\n",
    "                        json.dump(new_line, fw)\n",
    "                        fw.write('\\n')\n",
    "                        img_names = [img_path]\n",
    "                        seg_names = [seg] * 2\n",
    "def sup_odgt_format( img_dir, label_dir, dest_filename, extension):\n",
    "    with open(dest_filename + \".odgt\", \"w\") as fw:\n",
    "        for path in glob.glob(img_dir + \"*.\" + extension):\n",
    "            w, h = cv2.imread(path).shape[:2]\n",
    "            img_name = path.split(\"/\")[-1]\n",
    "            #print(img_name)\n",
    "            img_names = img_dir + img_name\n",
    "            seg_names = label_dir + img_name.split('.')[0] + '.png'\n",
    "            new_line = {}\n",
    "            new_line[\"fpath_img\"], new_line[\"fpath_segm\"], new_line[\"width\"], new_line[\"height\"] = \\\n",
    "                    img_names, seg_names, w, h\n",
    "            json.dump(new_line, fw)\n",
    "            fw.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-belarus",
   "metadata": {},
   "source": [
    " ### Get odgts for seqs, supervised, and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "insured-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_files = ['VOCallMinusVal1000FCM', 'VOCallMinusVal1000', 'VOCallMinusVal4096', 'VOCallMinusValFCM']\n",
    "dest_files = ['1000FCM', '1000', '4096', 'FCM']\n",
    "root_dir = \"/data/sara/DecompositionFeatureSegmentation/\"\n",
    "for ind, _file in enumerate(cluster_files):\n",
    "    ## only to get the pairs\n",
    "    cluster_file = \"/data/sara/DecompositionFeatureSegmentation/clustering/\" + _file\n",
    "    labeled_img_dir = \"/data/sara/semantic-segmentation-pytorch/datasets/VOC/train_imgs/\"    \n",
    "    all_img_dir = \"/data/sara/semantic-segmentation-pytorch/datasets/VOC/train_and_unlabeled_imgs/VOCallMinusVal/\"\n",
    "    labels_dir = \"/data/sara/semantic-segmentation-pytorch/datasets/VOC/train_labels/\"\n",
    "    odgt_format_from_dir(cluster_file, labeled_img_dir,all_img_dir ,labels_dir, root_dir+\"seqs_odgts/VOCpairs_\"+dest_files[ind], 'jpg')\n",
    "    \n",
    "    \n",
    "    ## only to get the train labeled ones\n",
    "    img_dir = \"/data/sara/semantic-segmentation-pytorch/datasets/VOC/train_imgs/\"\n",
    "    label_dir = \"/data/sara/semantic-segmentation-pytorch/datasets/VOC/train_labels/\"\n",
    "    sup_odgt_format(img_dir, label_dir, root_dir+\"supervised_odgts/VOCtrain_\"+dest_files[ind], 'jpg')\n",
    "    \n",
    "    ## only to get the val labeled ones\n",
    "    img_dir = \"/data/sara/semantic-segmentation-pytorch/datasets/VOC/val_imgs/\"\n",
    "    label_dir = \"/data/sara/semantic-segmentation-pytorch/datasets/VOC/val_labels/\"\n",
    "    sup_odgt_format(img_dir, label_dir, root_dir+\"val_odgts/VOCval_\"+dest_files[ind], 'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-integration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
